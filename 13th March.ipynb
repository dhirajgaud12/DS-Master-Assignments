{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6483cc1f-65af-44d3-bf44-ad645778a735",
   "metadata": {},
   "source": [
    "1) Assumptions of Anova- \n",
    "(1) samples are independent and random (2) normality, (3) absence of outliers, and (4) Homogenity of variance.\n",
    "Violation of any of these could lead to incorrect conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a4910-f580-4af3-99e9-82e75872ffbf",
   "metadata": {},
   "source": [
    "2) a) One way Anova - used when you have one categorical independent variable (factor) with more than two levels or groups, and you want to determine if there are any statistically significant differences among the means of these groups.\n",
    "b) Repeated measures ANOVA -  it is used when data is collected from the same subjects under multiple conditions or at different points in time.\n",
    "c) Factorial Anova- used in various fields of research and data analysis where researchers want to examine the combined effects of multiple independent categorical variables (factors) on a continuous dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c88f5a-7bfc-4604-9566-5f34967a5459",
   "metadata": {},
   "source": [
    "3) The partitioning of variance in Analysis of Variance (ANOVA) refers to the process of dividing the total variance in a dataset into different components that can be attributed to specific sources or factors. Understanding this concept is essential because it helps researchers or analysts gain insights into the variability within the data and provides a structured way to assess the significance of different factors or treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b5e589-4f51-4e84-baf1-b7824e9de66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST is  102.39999999999998  SSE is  102.39999999999998  SSR is  0\n"
     ]
    }
   ],
   "source": [
    "# 4) Calculating SST, SSR. SSE.\n",
    "import numpy as np\n",
    "data = [14, 16, 18, 22, 24, 26, 21, 23, 25, 27]\n",
    "overall_mean = np.mean(data)\n",
    "\n",
    "group_means = [19, 24, 26]\n",
    "group_sizes = [4, 3, 3]\n",
    "\n",
    "SSE = np.sum([group_sizes[i] * (group_means[i] - overall_mean) ** 2 for i in range(len(group_means))])\n",
    "\n",
    "SSR = np.sum([(x - group_means[i]) ** 2 for i, x in enumerate(group_means)])\n",
    "\n",
    "\n",
    "\n",
    "SST = SSR + SSE\n",
    "\n",
    "print (\"SST is \",SST,\" SSE is \",SSE,\" SSR is \",SSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06d39b9c-b433-4aad-886c-bf7415c18d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             df     sum_sq   mean_sq         F    PR(>F)\n",
      "C(Fertilizer)               1.0   0.033333  0.033333  0.012069  0.913305\n",
      "C(Watering)                 1.0   0.000369  0.000369  0.000133  0.990865\n",
      "C(Fertilizer):C(Watering)   1.0   0.040866  0.040866  0.014796  0.904053\n",
      "Residual                   28.0  77.333333  2.761905       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "#5)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.DataFrame({'Fertilizer': np.repeat(['daily', 'weekly'], 15),\n",
    "\t\t\t\t\t\t'Watering': np.repeat(['daily', 'weekly'], 15),\n",
    "\t\t\t\t\t\t'height': [14, 16, 15, 15, 16, 13, 12, 11,\n",
    "\t\t\t\t\t\t\t\t\t14, 15, 16, 16, 17, 18, 14, 13,\n",
    "\t\t\t\t\t\t\t\t\t14, 14, 14, 15, 16, 16, 17, 18,\n",
    "\t\t\t\t\t\t\t\t\t14, 13, 14, 14, 14, 15]})\n",
    "\n",
    "\n",
    "model = ols('height ~ C(Fertilizer) + C(Watering) +\\\n",
    "C(Fertilizer):C(Watering)',\n",
    "\t\t\tdata=dataframe).fit()\n",
    "result = sm.stats.anova_lm(model, type=2)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d6e5d-ba19-4b09-9cd1-419e37fb19a5",
   "metadata": {},
   "source": [
    "6) Given the F-Statistic: 5.23 suggests there is some variability in groups and P-Value: 0.02 indicates that the f-static as high as 5.23 under the null hypothesis is only 0.02 or 2%. And atleast one group is different from other in terms of variable.To understand which columns or groups are different we would further need post hoc tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf51a4a-b965-4ed8-994f-097522034e86",
   "metadata": {},
   "source": [
    "7) Handling missing data - \n",
    "    a) Listwise Deletion (Complete Case Analysis):\n",
    "        Method: In listwise deletion, cases with any missing data in any of the repeated measures are excluded from the analysis. This means that only cases with complete data for all time points are used. \n",
    "    Advantages: Simple and straightforward.\n",
    "    Disadvantages: Reduces sample size, potentially leading to reduced statistical power. May introduce bias if the missing data are not missing completely at random (MCAR). It can result in a loss of information.\n",
    "    \n",
    "    b) Pairwise Deletion (Available Case Analysis): In pairwise deletion, cases with missing data for a particular time point are excluded only from the analysis of that time point. All other available data points are used.\n",
    "    Advantages: Maximizes the use of available data.\n",
    "    Disadvantages: Analysis may involve different sample sizes at different time points, potentially complicating the interpretation of results. Like listwise deletion, it can introduce bias if data are not MCAR.\n",
    "    \n",
    "    c)Mean Imputation: Missing values are replaced with the mean of the observed values for that variable.\n",
    "    Advantages: Simple and does not reduce sample size.\n",
    "    Disadvantages: Can underestimate variability and lead to biased estimates if data are not MCAR. It reduces the variability within each group, potentially affecting the results of the ANOVA.\n",
    "    \n",
    "    d)Linear Interpolation:Missing values are estimated by linearly interpolating between adjacent observed values.\n",
    "    Advantages: Preserves the overall pattern of change over time.\n",
    "    Disadvantages: Assumes a linear relationship between time points, which may not be accurate. Linear interpolation can introduce noise, especially if the underlying relationship is not truly linear.\n",
    "    \n",
    "    e)Multiple Imputation: Multiple imputation involves creating multiple complete datasets by imputing missing values multiple times, incorporating uncertainty in the imputed values.\n",
    "    Advantages: Provides unbiased estimates under the missing at random (MAR) assumption. Preserves the variability in the data. Allows for valid statistical inference.\n",
    "    Disadvantages: Can be computationally intensive and may require specialized software.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90573d0b-66df-4de4-a99b-1d089c9e42af",
   "metadata": {},
   "source": [
    "8) \n",
    "\n",
    "Post-hoc tests are statistical tests conducted after the analysis of variance (ANOVA) to further investigate pairwise differences between groups when the ANOVA indicates that at least one group differs from the others. These tests help identify which specific groups are responsible for the observed differences. \n",
    "\n",
    "1) Tukey's Honestly Significant Difference (HSD) Test:\n",
    "Use: Tukey's HSD is widely used when you have performed an ANOVA and you want to compare all possible pairs of group means to determine which pairs are significantly different from each other.\n",
    "2) Scheffé's Test:\n",
    "Use: Scheffé's test is used when you have unequal sample sizes or variances among groups. It's more robust but less powerful than Tukey's HSD.\n",
    "3) Holm-Bonferroni Method:\n",
    "Use: This method is a stepwise correction for multiple comparisons. It orders the p-values from smallest to largest and compares them to adjusted significance levels.\n",
    "4) Sidak Correction:\n",
    "Use: Similar to Bonferroni, the Sidak correction is used for controlling the familywise error rate. It's less conservative than Bonferroni but still helps to avoid inflated Type I errors in multiple comparisons.\n",
    "5) Bonferroni Correction:\n",
    "Use: The Bonferroni correction is used when you have multiple pairwise comparisons to make, and it controls the familywise error rate (the probability of making at least one Type I error across all comparisons). It's more conservative but helps avoid false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c696c2-e2a6-49f5-9aee-91505835b735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reject the null hypothesis.\n",
      "F-statistic: 275.35\n",
      "P-value: 0.0000\n"
     ]
    }
   ],
   "source": [
    "#9) \n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "diet_A = np.array([2.5, 3.0, 2.8, 3.2, 2.9, 3.1, 2.7, 2.6, 3.2, 2.8, 3.0, 2.9, 2.7, 3.1, 3.3, 2.8, 2.9, 3.0, 2.6, 2.7, 3.1, 2.8, 2.9, 2.5, 3.0, 2.6, 2.8, 2.7, 3.2, 3.1, 2.9, 2.7, 3.0, 2.8, 2.6, 2.9, 3.1, 3.3, 3.0, 2.8, 2.7, 3.2, 3.1, 2.9, 3.0, 2.8, 2.6, 2.9])\n",
    "diet_B = np.array([2.2, 2.5, 2.1, 2.4, 2.3, 2.6, 2.2, 2.7, 2.5, 2.8, 2.4, 2.3, 2.1, 2.6, 2.4, 2.3, 2.5, 2.2, 2.7, 2.8, 2.6, 2.3, 2.4, 2.2, 2.1, 2.6, 2.4, 2.3, 2.5, 2.2, 2.7, 2.8, 2.6, 2.3, 2.4, 2.2, 2.1, 2.6, 2.4, 2.3, 2.5, 2.2, 2.7, 2.8, 2.6, 2.3, 2.4, 2.2])\n",
    "diet_C = np.array([1.8, 2.0, 1.9, 2.1, 1.7, 2.2, 1.8, 1.6, 2.0, 2.1, 1.9, 1.7, 2.2, 2.0, 1.8, 1.6, 2.1, 1.7, 1.9, 2.2, 2.0, 1.8, 1.9, 2.1, 1.7, 2.2, 1.6, 1.8, 2.0, 2.1, 1.9, 1.7, 2.2, 2.0, 1.8, 1.6, 2.1, 1.7, 1.9, 2.2, 2.0, 1.8, 1.9, 2.1, 1.7, 1.6, 2.0])\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Rejecting the null hypothesis.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "\n",
    "print(f\"F-statistic: {f_statistic:.2f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7931b5-996e-4717-a50b-9237f5849d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a significant main effect of Software.\n",
      "\n",
      "There is a significant main effect of Experience.\n",
      "\n",
      "There is no significant interaction effect between Software and Experience.\n"
     ]
    }
   ],
   "source": [
    "# 10) \n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Software': ['A', 'A', 'B', 'B', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C'],\n",
    "    'Experience': ['Novice', 'Experienced'] * 12, \n",
    "    'Time': [10, 12, 11, 14, 9, 11, 9, 11, 12, 13, 10, 13, 8, 10, 11, 13, 9, 10, 10, 12, 11, 14, 10, 13],\n",
    "})\n",
    "\n",
    "formula = 'Time ~ C(Software) + C(Experience) + C(Software):C(Experience)'\n",
    "model = ols(formula, data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "software_f_statistic = anova_table.loc['C(Software)', 'F']\n",
    "software_p_value = anova_table.loc['C(Software)', 'PR(>F)']\n",
    "\n",
    "if software_p_value < alpha:\n",
    "    software_result = \"There is a significant main effect of Software.\"\n",
    "else:\n",
    "    software_result = \"There is no significant main effect of Software.\"\n",
    "\n",
    "experience_f_statistic = anova_table.loc['C(Experience)', 'F']\n",
    "experience_p_value = anova_table.loc['C(Experience)', 'PR(>F)']\n",
    "\n",
    "if experience_p_value < alpha:\n",
    "    experience_result = \"There is a significant main effect of Experience.\"\n",
    "else:\n",
    "    experience_result = \"There is no significant main effect of Experience.\"\n",
    "\n",
    "interaction_f_statistic = anova_table.loc['C(Software):C(Experience)', 'F']\n",
    "interaction_p_value = anova_table.loc['C(Software):C(Experience)', 'PR(>F)']\n",
    "\n",
    "if interaction_p_value < alpha:\n",
    "    interaction_result = \"There is a significant interaction effect between Software and Experience.\"\n",
    "else:\n",
    "    interaction_result = \"There is no significant interaction effect between Software and Experience.\"\n",
    "\n",
    "print(software_result)\n",
    "print()\n",
    "print(experience_result)\n",
    "print()\n",
    "print(interaction_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "536fdb84-34eb-4b64-8eeb-a9f300bfef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to reject the null hypothesis.\n",
      "T-statistic: -1.67\n",
      "P-value: 0.0986\n"
     ]
    }
   ],
   "source": [
    "#11)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "np.random.seed(0) \n",
    "control_group = np.random.normal(loc=75, scale=10, size=50)  \n",
    "experimental_group = np.random.normal(loc=80, scale=10, size=50) \n",
    "\n",
    "t_statistic, p_value = stats.ttest_ind(control_group, experimental_group)\n",
    "alpha = 0.05\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Rejecting the null hypothesis.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "\n",
    "print(f\"T-statistic: {t_statistic:.2f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    data = pd.DataFrame({\n",
    "        'Score': np.concatenate((control_group, experimental_group)),\n",
    "        'Group': ['Control'] * 50 + ['Experimental'] * 50\n",
    "    })\n",
    "\n",
    "    posthoc = pairwise_tukeyhsd(data['Score'], data['Group'], alpha=alpha)\n",
    "    print(\"\\nPost-Hoc Tukey-Kramer Test:\")\n",
    "    print(posthoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99ac463f-ac64-42dc-b8b7-5f08a65e8787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejecting the null hypothesis.\n",
      "F-statistic: 4.41\n",
      "P-value: 0.0150\n",
      " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "=====================================================\n",
      "group1 group2 meandiff p-adj   lower    upper  reject\n",
      "-----------------------------------------------------\n",
      "     A      B -18.0665 0.3428 -48.7289 12.5959  False\n",
      "     A      C -38.1603 0.0107 -68.8227 -7.4979   True\n",
      "     B      C -20.0937 0.2673 -50.7561 10.5686  False\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#12)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "np.random.seed(0)  \n",
    "store_A_sales = np.random.normal(loc=500, scale=50, size=30)\n",
    "store_B_sales = np.random.normal(loc=520, scale=55, size=30)\n",
    "store_C_sales = np.random.normal(loc=490, scale=45, size=30)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Sales': np.concatenate((store_A_sales, store_B_sales, store_C_sales)),\n",
    "    'Store': ['A'] * 30 + ['B'] * 30 + ['C'] * 30\n",
    "})\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(store_A_sales, store_B_sales, store_C_sales)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Rejecting the null hypothesis.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "\n",
    "print(f\"F-statistic: {f_statistic:.2f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    posthoc = pairwise_tukeyhsd(data['Sales'], data['Store'], alpha=alpha)\n",
    "    print(posthoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10553cb-7175-4814-a859-cf6556a06177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
